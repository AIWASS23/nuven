\chapter{metodologia}\label{chapter:metodologia}

A metodologia adotada fundamenta-se na utilização e aplicação de algoritmos componentizados de uma arquitetura de rede neural convolucional. A construção do estudo foi organizada em múltiplas fases metodológicas, uma prática consolidada e bem fundamentada na pesquisa científica, especialmente nas áreas de aprendizado de máquina e análise de dados. Esta abordagem sistemática assegura uma organização clara e sequencial do processo de pesquisa, garantindo que cada fase seja meticulosamente planejada e executada. Conforme discutido por \cite{creswell}, uma estrutura metodológica bem definida permite maior rigor e reprodutibilidade na pesquisa. Adicionalmente, \cite{yin} enfatiza que a segmentação do estudo em etapas distintas facilita a análise e a interpretação dos resultados. Além disso, tal estrutura facilita a replicação do estudo e a verificação dos resultados por parte de outros pesquisadores, contribuindo para a robustez e a transparência científica.

\section{Pré análise dos dados}

O estudo iniciou-se com o carregamento e pré-processamento dos dados envolvendo a organização, análise e validação de um conjunto de dados de imagens médicas para classificação, abrangendo desde a leitura dos arquivos de imagem até a verificação de duplicatas e integridade dos dados.

Primeiramente, as imagens fornecidas foram descompactadas e o diretório principal que contém os dados das imagens foi definido. As classes de interesse foram estabelecidas como \textbf{Normal} e \textbf{Anormal}, com exemplos específicos identificados para cada categoria. Em seguida, foi realizado um percurso recursivo pelo diretório de dados para localizar arquivos de imagens com extensões específicas (.png, .jpg, .jpeg, .tif). Cada imagem foi classificada como \textbf{Normal} ou \textbf{Anormal} com base na pasta onde estava localizada. Após coletar os nomes dos arquivos e suas respectivas classes, os dados foram organizados em um \textbf{DataFrame} e exportados para um arquivo denominado \texttt{cell\_images\_classification.csv}. Após a geração do arquivo onde consta o nome da imagem e seu respectivo rótulo, verifico se cada imagem listada no arquivo possuía uma classe associada, caso contrário, uma mensagem foi exibida indicando a falta de associação. Para assegurar a integridade dos dados e evitar duplicatas, cada imagem foi processada para calcular seu \textbf{hash MD5}, sendo que duplicatas foram identificadas com base neste hash.

Essa etapa é extremamente essencial para o pré-processamento de dados de imagens médicas, desde a classificação inicial até a verificação de integridade e duplicatas tal abordagem é fundamental para manipulação, análise e processamento dos dados, demonstrando um processo estruturado para garantir a qualidade e a consistência dos dados utilizados em tarefas de aprendizado de máquina.


\section{Preparação do conjunto de dados}

O objetivo nessa etapa é carregar, pré-processar e preparar o conjunto de dados de imagens médicas para classificação utilizando TensorFlow e bibliotecas relacionadas. Neste trabalho, inicia-se carregando a tabela que contém os nomes dos arquivos de imagem e suas respectivas classes. Este arquivo é essencial para associar cada imagem ao seu rótulo correto durante o treinamento.

Logo em seguida, cada classe de imagem é mapeada para um número inteiro único, o que facilita o treinamento de modelos de aprendizado de máquina. Esse mapeamento é criado a partir das classes únicas encontradas nessa tabela. Sequencialmente, percorro o diretório principal e suas subpastas em busca de arquivos de imagem no formato \textbf{.tif}, onde cada imagem é carregada e redimensionada para um tamanho padrão e convertida em um array NumPy. Por fim, carregam-se todas as imagens e associam-se corretamente suas classes, os dados são convertidos para arrays NumPy, facilitando a manipulação e uso subsequente nos modelos de aprendizado de máquina.

Esta etapa é um processo sistemático para preparar dados das imagens médicas para tarefas de classificação. Cada etapa, desde o carregamento das imagens até a preparação dos dados, é cuidadosamente implementada para garantir a consistência e a eficácia do modelo final, sendo essencial para a construção de modelos robustos e precisos.

\section{Treino e Teste}

Primeiramente, destaco que a divisão entre treino e teste visa avaliar fundamentalmente a capacidade de generalização do modelo, isto é, sua habilidade de realizar previsões precisas em dados não observados anteriormente. Nesse sentido, é crucial seguir as orientações de \cite{matlab} para uma avaliação apropriada dos algoritmos de visão computacional, que enfatizam a necessidade de dividir os dados em conjuntos de treinamento e teste, assegurando a capacidade do modelo de generalizar para novos exemplos. Para isso, adotei a prática comum de particionar o conjunto de dados em dois subconjuntos mutuamente exclusivos: o conjunto de treinamento, utilizado para treinar o modelo, e o conjunto de teste, empregado para avaliar o desempenho do modelo em dados não observados.

No contexto deste estudo, os dados foram divididos em conjuntos de treino e teste utilizando a função \textbf{train\_test\_split} do scikit-learn. Essa divisão é essencial para evitar que o modelo se ajuste demais aos dados de treinamento, permitindo uma avaliação confiável do desempenho em dados não vistos. Optou-se por alocar 80\% das imagens para treinamento e 20\% para teste. Além disso, o parâmetro \textbf{random\_state = 42} garante a reprodutibilidade dos resultados. Isso significa que, em diferentes execuções do código, a mesma divisão entre os conjuntos de treino e teste será obtida, facilitando a comparação consistente entre diferentes modelos e abordagens.

Finalmente, as intensidades dos pixels das imagens são normalizadas para o intervalo [0, 1], garantindo que as características das imagens tenham uma escala comum e as labels de classe são convertidas para o formato one-hot encoding usando a função \textbf{to\_categorical}, tais estratégias são benéficas para o treinamento eficaz do modelo e classificação multiclasse.



\section{Modelo}

Conforme mencionado por \cite{geetha}, a construção de um modelo é uma arte delicada, onde cada variável e sua interação devem ser consideradas com cuidado, como peças em um quebra-cabeça, para formar um todo coeso e preditivo. Assim, a análise comparativa desses modelos em diferentes cenários se torna crucial para determinar a estratégia mais eficaz para a aplicação em questão.

O modelo de rede neural é configurado utilizando a API Sequential do TensorFlow Keras, conhecida por permitir a construção de modelos camada a camada de maneira sequencial. Este tipo de abordagem é especialmente útil para a criação de arquiteturas robustas e eficazes, como a definida neste contexto. A camada de entrada é definida por \textbf{Input()}, onde shape é definido como 50 representando a dimensionalidade das imagens de entrada e 3 denotando a quantidade de canais de cor.

O modelo inclui um total de cinco camadas convolucionais, seguidas por camadas de \textbf{max-pooling} para redução de dimensionalidade, camadas de \textbf{dropout} para regularização e camadas totalmente conectadas para a extração de características das imagens e classificação subsequente. A função de ativação \textbf{relu6} é especificada para todas as camadas convolucionais e totalmente conectadas. Esta função é uma variante da função de ativação ReLU (Rectified Linear Unit), limitada superiormente a 6 para mitigar problemas de explosão de gradientes, frequentemente observados em redes profundas.

Cada camada \textbf{Conv2D} é configurada com um determinado número de filtros e um tamanho de kernel específico, aplicando a função de ativação para introduzir não-linearidades na rede. As camadas \textbf{MaxPooling2D} são empregadas após as camadas convolucionais para realizar subamostragem, preservando as características mais proeminentes das imagens. As camadas de \textbf{dropout} são inseridas com taxas de 25\% e 50\%, respectivamente, após as camadas convolucionais e densas. Essas camadas ajudam a prevenir o overfitting ao desativar aleatoriamente uma fração das unidades de saída durante o treinamento.

A camada \textbf{Flatten()} é utilizada para transformar a saída das camadas convolucionais em um vetor unidimensional, preparando os dados para serem processados por camadas densas subsequentes. A camada \textbf{Dense()} representa a camada de saída da rede, com número de neurônios correspondentes ao número de classes no problema de classificação e a função de ativação softmax gerando as probabilidades para cada classe, garantindo que a soma das saídas seja igual a 1 e facilitando a interpretação da saída como probabilidades de pertencimento a cada classe.

